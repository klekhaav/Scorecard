{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#--- Test it ---\\nparse_date(\"snapshots/HealthData.gov_2014-02-24_data.json\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_date(file_name):\n",
    "    \n",
    "    starting_point_of_date = \"_20\"\n",
    "    date_pos_start = file_name.find(starting_point_of_date)+1\n",
    "    return file_name[date_pos_start:date_pos_start+10]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "#--- Test it ---\n",
    "parse_date(\"snapshots/HealthData.gov_2014-02-24_data.json\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# --- Experiment with keys ---\\nif \"bureauCode\"  in json_data_list[0][0]: print json_data_list[0][0][\"bureauCode\"]\\nif \"publisher\"   in json_data_list[0][0]: print json_data_list[0][0][\"publisher\"]\\nif \"landingPage\" in json_data_list[0][0]: print json_data_list[0][0][\"landingPage\"]\\nprint json.dumps(json_data_list[0][0], sort_keys=False, indent=4)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Pull out the most important elements to tally on\n",
    "def get_keys(dataset):\n",
    "    keys = [\"bureauCode\", \"programCode\", \"publisher\", \n",
    "            \"landingPage\",\"modified\",\n",
    "            \"Identifier\", \"downloadURL\"]\n",
    "    '''\n",
    "    Characteristics of non-federal entries for DKAN\n",
    "    → Publisher:Name is \"State of\" or \"City of\"\n",
    "    → downloadURL has non-hhs domain\n",
    "    → Identifier has non-hhs domain\n",
    "    → Usually \"bureauCode\": [\"009:00\"  and \"programCode\": [ \"009:000\"\n",
    "    '''\n",
    "    key_values = []\n",
    "    for i,key in enumerate(keys):\n",
    "        if key in dataset:\n",
    "            key_values.append(dataset[key])\n",
    "        else:\n",
    "            key_values.append(None)\n",
    "    return dict(zip(keys, key_values))\n",
    "        \n",
    "\n",
    "'''\n",
    "------------------------------------------------\n",
    "---  Capturing agency counts                 ---\n",
    "------------------------------------------------\n",
    "Many dataset entries lack bureauCode.  \n",
    "So perhaps other identifiers can be used as a proxy\n",
    "'''\n",
    "'''\n",
    "#--- Test it ---\n",
    "print get_keys(json_data_list[0][0])\n",
    "'''\n",
    "'''\n",
    "# --- Experiment with keys ---\n",
    "if \"bureauCode\"  in json_data_list[0][0]: print json_data_list[0][0][\"bureauCode\"]\n",
    "if \"publisher\"   in json_data_list[0][0]: print json_data_list[0][0][\"publisher\"]\n",
    "if \"landingPage\" in json_data_list[0][0]: print json_data_list[0][0][\"landingPage\"]\n",
    "print json.dumps(json_data_list[0][0], sort_keys=False, indent=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# FIXME: Code not yet finished\n",
    "# FIXME: Should call get_keys\n",
    "# Create a dictionary of values for comparison\n",
    "\n",
    "def get_key_list(dataset_list):\n",
    "    key_list = []\n",
    "    for index, dataset in enumerate(dataset_list):\n",
    "        key_list.append(get_keys(dataset))\n",
    "    #for # List of unique bureauCode values    \n",
    "    \n",
    "    totals = len(dataset_list)\n",
    "    #print get_keys(dataset[0])\n",
    "    return key_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n#--- Test it ---\\nprint support_old_schema(dataset)\\n\\n'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def support_old_schema(dataset_list):\n",
    "    if isinstance(dataset_list, dict):\n",
    "        return dataset_list[\"dataset\"]\n",
    "    elif isinstance(dataset_list, list):\n",
    "        return dataset_list\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "'''    \n",
    "#--- Test it ---\n",
    "print support_old_schema(dataset)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob   # Wildcard search\n",
    "import json\n",
    "\n",
    "def load_file(file_name):\n",
    "    with open(file_name) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        return json_data\n",
    "        print(\"Loaded file: \"+file_name)\n",
    "\n",
    "\n",
    "#FIXME: Need to find agency decode\n",
    "#       Examples: {'009:25', '009:15', '009:92', '009:10', '009:75',\n",
    "#                  '009:20', '009:30', '009:17', '009:70', '009:00', \n",
    "#                  '009:38', '009:33'}\n",
    "def main():\n",
    "    file_pattern = \"snapshots/\"\n",
    "    file_pattern += \"HealthData.gov[_][0-9][0-9][0-9][0-9][-][0-9][0-9][-][0-9][0-9][_]data.json\"\n",
    "    file_name_list = glob.glob(file_pattern)\n",
    "\n",
    "    datasets = []\n",
    "    for index, file_name in enumerate(reversed(file_name_list)):\n",
    "\n",
    "        snapshot_date = parse_date(file_name)\n",
    "        dataset_list = load_file(file_name)\n",
    "        dataset_list = support_old_schema(dataset_list)\n",
    "\n",
    "        key_list      = get_key_list(dataset_list)\n",
    "        agency_counts = get_agency_counts(key_list)\n",
    "\n",
    "        print snapshot_date+\": \"+str(agency_counts)+\"\\n\"\n",
    "\n",
    "        if index > 0: break  # Don't run all for debugging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-01: {'009:25': 44, '009:15': 19, '009:92': 7, '009:10': 151, '009:75': 6, '009:20': 243, '009:30': 11, '009:17': 1, '009:70': 81, '009:00': 679, '009:38': 551, '009:33': 14}\n",
      "\n",
      "2015-11-30: {'009:25': 44, '009:15': 19, '009:92': 7, '009:10': 151, '009:75': 6, '009:20': 243, '009:30': 11, '009:17': 1, '009:70': 81, '009:00': 679, '009:38': 551, '009:33': 14}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#======================\n",
    "#== This section for main()\n",
    "#======================\n",
    "# FIXME: Change order to save each row immediately, rather than all at end\n",
    "\n",
    "file_pattern = \"snapshots/\"\n",
    "#file_pattern += \"*2015-02-01*\"\n",
    "#file_pattern += \"*2015-06-18*\"\n",
    "file_pattern += \"HealthData.gov[_][0-9][0-9][0-9][0-9][-][0-9][0-9][-][0-9][0-9][_]data.json\"\n",
    "file_name_list = glob.glob(file_pattern)\n",
    "\n",
    "datasets = []\n",
    "agency_lookup = load_agency_lookup()\n",
    "dict_counts_by_date = {}\n",
    "\n",
    "\n",
    "for index, file_name in enumerate(reversed(file_name_list)):\n",
    "\n",
    "    snapshot_date = parse_date(file_name)\n",
    "    dataset_list = load_file(file_name)\n",
    "    dataset_list = support_old_schema(dataset_list)\n",
    "\n",
    "    key_list      = get_key_list(dataset_list)\n",
    "    #print key_list\n",
    "    #print agency_lookup\n",
    "    agency_counts = get_agency_counts(key_list,agency_lookup)\n",
    "    \n",
    "    #print snapshot_date+\": \"+str(agency_counts)+\"\\n\"\n",
    "    \n",
    "    \n",
    "    dict_counts_by_date[snapshot_date]=agency_counts\n",
    "\n",
    "    #if index > 15: break  # Don't run all for debugging\n",
    "\n",
    "#print agency_lookup\n",
    "#dict_counts_by_date\n",
    "convert_dict_to_csv(dict_counts_by_date,agency_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-02-24\n"
     ]
    }
   ],
   "source": [
    "print snapshot_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DHHS',\n",
       " 'FDA',\n",
       " 'HRSA',\n",
       " 'IHS',\n",
       " 'CDC',\n",
       " 'NIH',\n",
       " 'SAMHSA',\n",
       " 'AHRQ',\n",
       " 'CMS',\n",
       " 'ACF',\n",
       " 'ACL',\n",
       " 'GDM',\n",
       " 'PSC',\n",
       " 'OIG',\n",
       " 'State',\n",
       " 'City',\n",
       " 'Other']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_agency_abbrev_list(agency_lookup):\n",
    "\n",
    "    # Looks more complex than needed, but due to sorting by key\n",
    "    bureau_code_list = []\n",
    "    for bureau_code in agency_lookup.iterkeys():\n",
    "        bureau_code_list.append(bureau_code) \n",
    "    bureau_code_list.sort()\n",
    "\n",
    "    agency_abbrev_list = []\n",
    "    for bureau_code in bureau_code_list:\n",
    "        agency_abbrev_list.append(agency_lookup[bureau_code]) \n",
    "        \n",
    "    return agency_abbrev_list\n",
    "    \n",
    "\n",
    "#agency_abbrev_list = get_agency_abbrev_list(agency_lookup)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def convert_dict_to_csv(dict_counts_by_date,agency_lookup):\n",
    "    \n",
    "    # --- Be sure list of abbreviations is sorted by key ---\n",
    "    agency_abbrev_list = get_agency_abbrev_list(agency_lookup)\n",
    "\n",
    "    \n",
    "    row_csv = []\n",
    "    row_csv_list = []\n",
    "    \n",
    "    # --- Build header ---\n",
    "    row_csv.append(\"Date\")\n",
    "    for agency_abbrev in agency_abbrev_list:\n",
    "        row_csv.append(agency_abbrev)\n",
    "    row_csv_list.append(row_csv)\n",
    "    #print row_csv\n",
    "\n",
    "    for row_date,row_counts in dict_counts_by_date.iteritems():\n",
    "        row_csv = []\n",
    "        row_csv.append(row_date)\n",
    "\n",
    "        # Using this method because want to be sorted by bureau_code\n",
    "        for agency_abbrev in agency_abbrev_list:\n",
    "            row_csv.append(str(row_counts.get(agency_abbrev,0)))\n",
    "\n",
    "        row_csv_list.append(row_csv)\n",
    "        #print row_csv\n",
    "\n",
    "        \n",
    "    with open(\"generated/totals_by_agency.csv\", \"wb\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerows(row_csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_agency_counts(key_list,agency_lookup):\n",
    "    agency_counts = {}\n",
    "    for index,key_item in enumerate(key_list):\n",
    "\n",
    "        agencies = key_item[\"bureauCode\"]\n",
    "        #print key_item\n",
    "        #print key_item[\"publisher\"][\"name\"]\n",
    "\n",
    "\n",
    "        # Just in case it's not a list, make it one\n",
    "        agencies = agencies if isinstance(agencies,list) else [agencies]\n",
    "\n",
    "        #print agencies\n",
    "        for agency in agencies:\n",
    "            #agency = agency.encode('ascii','ignore')\n",
    "            agency_abbrev = agency_lookup.get(agency,\"Other\")\n",
    "            \n",
    "            # Occassionally \"bureauCode\"][0] == \"009:00\" is used for State/Local\n",
    "            if agency == \"009:00\":\n",
    "                \n",
    "                publisher_name = key_item[\"publisher\"]\n",
    "                # Handle when publisher is not a dictionary\n",
    "                if isinstance(publisher_name, dict): publisher_name = str(publisher_name)\n",
    "               \n",
    "                if \"State of\" in publisher_name:\n",
    "                    agency_abbrev = \"State\"\n",
    "                elif \"City of\" in publisher_name:\n",
    "                    agency_abbrev = \"City\"\n",
    "\n",
    "            agency_counts[agency_abbrev] = agency_counts.get(agency_abbrev, 0) + 1\n",
    "           \n",
    "        #if index > 0: break  # Don't run all for debugging\n",
    "\n",
    "            \n",
    "    return agency_counts\n",
    "\n",
    "\n",
    "#agency_lookup = load_agency_lookup()\n",
    "#agency_counts = get_agency_counts(key_list,agency_lookup)\n",
    "#print agency_counts\n",
    "#print snapshot_date+\": \"+str(agency_counts)+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'009:25': 'NIH', u'009:90': 'GDM', u'009:91': 'PSC', u'009:92': 'OIG', u'009:38': 'CMS', u'009:10': 'FDA', u'009:75': 'ACL', u'009:20': 'CDC', u'009:30': 'SAMHSA', u'009:17': 'IHS', u'009:70': 'ACF', u'009:00': 'DHHS', u'009:33': 'AHRQ', u'009:15': 'HRSA'}\n"
     ]
    }
   ],
   "source": [
    "def load_agency_lookup():\n",
    "\n",
    "    with open('agency_lookup_columns.json') as data_file:    \n",
    "        agency_lookup_columns = json.load(data_file)\n",
    "\n",
    "    bureau_code_index = agency_lookup_columns['columns'].index('bureau_code')\n",
    "    agency_abbrev_index = agency_lookup_columns['columns'].index('agency_abbrev')\n",
    " \n",
    "    agency_lookup = {}\n",
    "   \n",
    "    for agency_record in agency_lookup_columns['data']:\n",
    "        # TBD: May want to convert unicode using  .encode('ascii','ignore')\n",
    "        agency_lookup[agency_record[bureau_code_index]] = agency_record[agency_abbrev_index].encode('ascii','ignore')\n",
    "\n",
    "    return agency_lookup\n",
    "\n",
    "\n",
    "#print load_agency_lookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "'''\n",
    "------------------------------------------------\n",
    "---  Reload the file only if it changed\n",
    "------------------------------------------------\n",
    "'''\n",
    "def get_csv_data(last_mtime = 0, csv_data = []):\n",
    "    CSV_FILE_NAME = \"generated/totals_by_agency.csv\"\n",
    "\n",
    "    try:\n",
    "        mtime = os.path.getmtime(CSV_FILE_NAME)\n",
    "    except OSError:\n",
    "        mtime = 0\n",
    "\n",
    "    # Reload if there's a newer file\n",
    "    if mtime > last_mtime:\n",
    "        last_mtime = mtime\n",
    "\n",
    "        csv_file = open(CSV_FILE_NAME)\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        \n",
    "        csv_data = []\n",
    "        for index, row in enumerate(csv_reader):\n",
    "            csv_data.append(row)\n",
    "            #if index > 0: break\n",
    "\n",
    "    return (mtime, csv_data)\n",
    "\n",
    "\n",
    "#mtime, csv_data = get_csv_data()\n",
    "#print mtime\n",
    "#print csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1452231080.0\n",
      "[['Date', 'DHHS', 'FDA', 'HRSA', 'IHS', 'CDC', 'NIH', 'SAMHSA', 'AHRQ', 'CMS', 'ACF', 'ACL', 'GDM', 'PSC', 'OIG', 'State', 'City', 'Other'], ['2014-06-30', '61', '81', '17', '1', '81', '44', '40', '13', '616', '81', '6', '0', '0', '0', '0', '0', '559']]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "------------------------------------------------\n",
    "---  Check data from CSV                     \n",
    "------------------------------------------------\n",
    "'''\n",
    "#: First time set to zero\n",
    "last_mtime = 0\n",
    "csv_data = []\n",
    "\n",
    "'''\n",
    "#: Alternative, if not setting  first time\n",
    "last_mtime = 0 if not ('last_mtime' in locals()) else last_mtime\n",
    "csv_data = [] if not ('csv_data' in locals()) else csv_data\n",
    "'''\n",
    "\n",
    "\n",
    "#: Get data from file and update timestamp\n",
    "last_mtime, csv_data = get_csv_data(last_mtime, csv_data)\n",
    "\n",
    "#: Display data\n",
    "csv_dates = []\n",
    "header = csv_data[0]\n",
    "date_pos = header.index('Date')\n",
    "for index, row in enumerate(csv_data[1:]):\n",
    "    csv_dates.append(row[date_pos])\n",
    "    #print(row)\n",
    "    #if index > 10: break\n",
    "\n",
    "#print csv_dates        \n",
    "#print csv_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_name_list():\n",
    "    file_pattern = \"snapshots/\"\n",
    "    file_pattern += \"HealthData.gov[_][0-9][0-9][0-9][0-9][-][0-9][0-9][-][0-9][0-9][_]data.json\"\n",
    "    file_name_list = glob.glob(file_pattern)\n",
    "    \n",
    "    return file_name_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "level": 1,
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "# Header\n",
    "Documentation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2014-06-30']\n"
     ]
    }
   ],
   "source": [
    "def get_csv_date_list(csv_data):\n",
    "    \n",
    "    csv_date_list = []\n",
    "    header = csv_data[0]\n",
    "    date_pos = header.index('Date')\n",
    "    \n",
    "    for index, row in enumerate(csv_data[1:]):\n",
    "        csv_date_list.append(row[date_pos])\n",
    "    \n",
    "    return csv_date_list\n",
    "\n",
    "\n",
    "#print get_csv_date_list(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "------------------------------------------------\n",
    "---  Load only dates missing                    \n",
    "------------------------------------------------\n",
    "'''\n",
    "\n",
    "file_name_list = get_file_name_list()\n",
    "mtime, csv_data = get_csv_data()\n",
    "csv_date_list = get_csv_date_list(csv_data)\n",
    "\n",
    "\n",
    "datasets = []\n",
    "agency_lookup = load_agency_lookup()\n",
    "dict_counts_by_date = {}\n",
    "\n",
    "#: Load missing dates\n",
    "for index, file_name in enumerate(reversed(file_name_list)):\n",
    "    snapshot_file_date = parse_date(file_name)\n",
    "    if snapshot_file_date not in csv_dates:\n",
    "        print \"Running date: \"+snapshot_file_date\n",
    "\n",
    "        dataset_list = load_file(file_name)\n",
    "        dataset_list = support_old_schema(dataset_list)\n",
    "\n",
    "        key_list      = get_key_list(dataset_list)\n",
    "        #print key_list\n",
    "        #print agency_lookup\n",
    "        agency_counts = get_agency_counts(key_list,agency_lookup)\n",
    "\n",
    "        #print snapshot_date+\": \"+str(agency_counts)+\"\\n\"\n",
    "\n",
    "\n",
    "        dict_counts_by_date[snapshot_date]=agency_counts\n",
    "\n",
    "        #if index > 15: break  # Don't run all for debugging\n",
    "\n",
    "#print agency_lookup\n",
    "#dict_counts_by_date\n",
    "convert_dict_to_csv(dict_counts_by_date,agency_lookup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 4,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
